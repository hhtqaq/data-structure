## 散列表（哈希表）的查找

已知的几种查找方法：
顺序查找  O(N)
二分查找（静态查找）  O(logN)
二叉搜索树      O(h)  h为二叉树高度   （动态查找：有插入有删除有查找）
平衡二叉树      O(logN)
红黑树             O(logN)

&emsp;&emsp;这类查找方法都是以关键字的比较为基础的。在查找过程中只考虑各元素关键字之间的
相对大小，记录在存储结构中的位置和去关键字无直接关系，其查找时间与表的长度有关，特别是当
结点个数很多时，查找时要大量地与无效结点的关键字进行比较，致使查找的速度很慢。如果找到元素
存储位置和关键词的关系，通过这种关系直接找到相应的记录，这就是散列查找法的思想。

### 1 散列表的相关概念

 - (1) **散列函数和散列地址**：在记录的存储位置p和其关键字key之间建立一个确定的对应关系H，使
 p=H(key),称这个对应关系H为散列函数，p为散列地址。
 - (2) **散列表**：一个有连续的地址空间，用以存储散列函数计算得到相应散列地址的数据记录。通常
 散列表的存储空间是一个一位数组，散列地址是数组的下标。
 - (3) **冲突和同义词**：对不同的关键词可能得到同一散列地址，即key1 != key2,而H(key1)=H(key2),
 这种现象称为冲突。key1与key2互称为同义词。

&emsp;&emsp;综上所述，散列查找法主要研究以下两方面的问题：

&emsp;&emsp;（1）如何构造好的散列函数。（冲突越低越好）

&emsp;&emsp;（2）如何处理冲突。（无法避免如何解决）

### 2 散列函数的构造方法
&emsp;&emsp; 构造散列函数的方法很多，一般来说，应根据具体问题选用不同的散列函数，通常要考虑以下
因素：
 - (1) 散列表的长度
 - (2) 关键字的长度
 - (3) 关键字的分布情况
 - (4) 计算散列函数所需的时间
 - (5) 记录的查找频率
 
构造一个“好”的散列函数应遵循以下两条原则：
 
 - (1) 函数计算要简单，每一关键字只能有一个散列地址与之对应。
 - (2) 函数的值域需在表长的范围内，计算出的散列地址分布应均匀，尽可能减少冲突。
 
&emsp;&emsp; 下面介绍构造散列函数几种常用的方法。
 
#### 2.1  数字分析法
&emsp;&emsp; 如果事先知道关键字集合，且每个关键字的位数比散列表的地址码位数多，每个关键字由n位
数组成，如k1k2...kn,则可以从关键字中提取数字分布比较均匀的若干位作为散列地址。
#### 2.2 平方取中法
&emsp;&emsp; 通常在选定散列函数时不一定能知道关键字的全部情况，取其中哪几位也不一定合适，而一
个数平方后的中间几位数和数的每一位都相关，如果取关键字平方后的中间几位或其组合作为散列地址，
则使随机分布的关键字得到的散列地址也是随机的，具体所取的位数由表长决定。平方取中法是一种较
常用的构造散列函数的方法。<br/>
&emsp;&emsp;平方取中法的使用情况：不能事先了解关键字的所有情况，或难于直接从关键字中找到取值
较分散的几位。
#### 2.3 折叠法
&emsp;&emsp;将关键词分割成位数相同的几部分（最后一部分的位数可以不同），然后取这几部分的叠加
和（舍去进位）做为散列地址，这种方法成为折叠法。<br/>
&emsp;&emsp;折叠法的适用情况：适用于散列地址的位数较少，而关键字的位数较多，且难于直接从关键字
中找到取值较分散的几位。
#### 2.4 除留余数法（**重点**）
&emsp;&emsp;假设散列表表长为m，选择一个不大于m的数p，用p去除关键字，除后所得余数为散列地址，即
_**H(key)=key mod p**_<br/>
&emsp;&emsp;这个方法的关键是选取适当的p，一般情况下,可以选p为小于表长的最大质数。例如表长m=100,
可以取p=97.可能有人看到这里会跟我一样有个疑问，为什么p不取100，取97不是浪费了后面的空间吗？所以
我去查了一下，看到了这篇文章https://blog.csdn.net/Ilozk/article/details/90762541。<br/>
&emsp;&emsp;除留余数法计算简单,适用范围广，是最常用的构造散列函数的方法。它不仅可以对关键字直接
取模，也可在折叠，平方取中等运算之后取模，这样能够保证散列地址一定落在散列表的地址空间中。

&emsp;&emsp;说到这里，谈一下java中的HashMap使用的散列（hash）算法

#### 2.5 散列函数在HashMap中的应用（jdk1.8）

&emsp;&emsp;首先，HashMap使用的也是除留余数法，只是与上诉所说的实现上稍微有点不同。源码如下：

        //这里HashMap初始还没分配数组大小 在putval才初始化  懒加载
        n = (tab = resize()).length;
        //n=数组长度 且为2的幂次方 >=16  hash是一个很大的数字
        if ((p = tab[i = (n - 1) & hash]) == null)
        
&emsp;&emsp; **_(n - 1) & hash_<==>hash%n** ,HashMap这么做是为了更加高效，因为计算机中位运
算的速度大于取余运算。接下来我们仔细分析一下为什么这样可以？或者说"&"运算等于"%"的条件是
什么？**_n为2的幂次方_**，仅此而已。（这也是HashMap每次扩容都为2的次幂原因）

&emsp;&emsp;二进制&运算：<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;n=15 = _0000 0000 0000 0000 0000 0000 0000_ **1111** <br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;  & <br />
&emsp;&nbsp;&nbsp;hash=954508689 = _1001 1000 1110 0100 1010 0101 1001_ **0001** <br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;结果为 ： _0000 0000 0000 0000 0000 0000 0000_ **0001** = 1   <br />
&emsp;&emsp;假如hash值变成954508690，结果就为0010=2，可以发现只有低位参与了运算，高位为0，并且随着hash值
不同，低位不同，结果也不同，并总能取满所有位置。   
 
 &emsp;&emsp;上面说到过除数尽量取小于数组大小的质数，这样可以减少hash冲突，但是显然HashMap是违背了的，
 它的除数是2的幂次方，那么HashMap是怎么解决这个问题的呢？
 
         //在前面提到的hash值计算是通过下面这个hash()方法
        static final int hash(Object key) {
                int h;
                return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
          }
          
&emsp;&emsp;这个方法有两个地方需要注意（也就是HashMap优化的地方）：
 - key.hashCode()<br />
  
 &emsp;&emsp;我们知道Object类是所有类的父类，他有一个hashCode()方法，子类可以覆盖。
 
 String类的hashCode()方法：
 
     public int hashCode() {
            int h = hash;
            if (h == 0 && value.length > 0) {
                char val[] = value;
    
                for (int i = 0; i < value.length; i++) {
                    h = 31 * h + val[i];
                }
                hash = h;
            }
            return h;
      }
    
 自定义类的1.7+自动生成的hashCode()方法
 
     @Override
      public int hashCode() {
             return Objects.hash(id, username, password);
       }
       
       //实际上是这个方法
      public static int hashCode(Object a[]) {
               if (a == null)
                   return 0;
       
               int result = 1;
       
               for (Object element : a)
                   result = 31 * result + (element == null ? 0 : element.hashCode());
       
               return result;
       }

 1) 所有的字符都参与了计算hashcode的运算，这样即使一个字符改变hash值也会改变很大。<br />
 2) 为什么使用乘数31，总结如下：
 
    1) 31是奇素数。
    2) 哈希分布较为均匀。偶数的冲突率基本都很高，只有少数例外。较小的乘数，冲突率也比较高，如1至20。
    3) 哈希计算速度快。可用移位和减法来代替乘法。现代的JVM可以自动完成这种优化，如31 * i = (i << 5) - i。
    4) 31和33的计算速度和哈希分布基本一致，整体表现好，选择它们就很自然了。
    
 
  -  扰动函数 ^ (h >>> 16)
  
 &emsp;&emsp;理论上散列值是一个int型，如果直接拿散列值作为下标访问HashMap数组的话，考虑到2进制32位带符号
的int表值范围从-2147483648到2147483648。前后加起来大概40亿的映射空间。只要哈希函数映射得比较
均匀松散，一般应用是很难出现碰撞的。

 &emsp;&emsp;但问题是一个40亿长度的数组，内存是放不下的。你想，HashMap扩容之前的数组初始大小才16。所以这
个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来访问数组下标。上面只用到了低位。

 &emsp;&emsp;但这时候问题就来了，这样就算我的散列值分布再松散，要是只取最后几位的话，碰撞也会很严重。更要命的
是如果散列本身做得不好，分布上成等差数列的漏洞，恰好使最后几个低位呈现规律性重复，就无比蛋疼。

 &emsp;&emsp;这时候“扰动函数”的价值就体现出来了，说到这里大家应该猜出来了。看下面这个图，
 
 ![Image text](https://github.com/hhtqaq/data-structure/raw/master/myHashSearch/img-file/1.png)

 &emsp;&emsp;右位移16位，正好是32bit的一半，自己的高半区和低半区做异或，就是为了混合原始哈希码的高位和低位，
以此来加大低位的随机性。而且混合后的低位掺杂了高位的部分特征，这样高位的信息也被变相保留下来。

### 3 处理冲突的方法
&emsp;&emsp;选择一个“好”的散列（hash）函数可以在一定程度上减少冲突，但在实际应用中，很难完全
避免发生冲突，所以选择一个有效处理冲突的方法是散列法的另一个关键问题。

&emsp;&emsp;处理冲突的方法与散列表本身的组织形式有关。按照组织形式的不同，通常分为两大类：
**_开放地址法_**和**_链地址法_**。

#### 3.1 开放地址法
&emsp;&emsp;开放地址发的基本思想是：把记录都存储在散列表数组中，当某一记录关键字key的初始散列
地址H0=H(key)发生冲突时，以H0为基础，采取合适方法计算得到另一个地址H1，如果H1仍然发生冲突，以
H1位基础再求下一个地址H2，若H2仍然冲突，再求得H3.依次类推，直至Hk不发生冲突为止，则Hk为该记录
在表中的散列地址。

&emsp;&emsp;这种方法在寻找“下一个”空的散列地址时，原来的数组空间对所有的元素都是开放的，所以
称为开放地址法。通常把寻找“下一个”空位的过程称为探测，上述方法可用如下公式表示: <br/>
<center>Hi=（H(key)+di）%m    i=1,2,...k(k<=m-1)</center>

&emsp;&emsp;其中H(key)为散列函数，m为散列表表长，di为增量序列。根据di取值的不同，可以分为以下
三种探测方法：
 - 线性探测法:发生冲突就找下一个位置。
 
 <center>di=1,2,3,...m-1</center>
 
 - 二次探测法 
 
 <center>di=1²，-1²，2²，-2²，...,k²，-k²(k<=m/2)</center>
 
 - 伪随机探测法 
 
 <center>伪随机数序列</center>

&emsp;&emsp;以上三种处理方法各有优缺点。线性探测法的优点是：只要散列表未填满，总能找到一个不
发生冲突的地址；缺点是：会产生“二次聚集”（两个第一个散列地址不同争夺同一个后继散列地址）现象。
而二次探测法和伪随机探测法的优点是：可以避免“二次聚集”现象；缺点也很显然：不能保证一定找到不
发生冲突的地址。

#### 3.2 链地址法 （_**重点**_）
&emsp;&emsp;链地址法的基本思想是：把具有相同散列地址的记录放在用一个单链表中，称为同义词链表。

#### 3.3 开放地址法和链地址法比较

 ![Image text](https://github.com/hhtqaq/data-structure/raw/master/myHashSearch/img-file/1.png)

#### 3.4 HashMap解决处理冲突方法 （_**重点**_）

&emsp;&emsp;HashMap在jdk1.7中使用的也是链地址法，但是在1.8改成了，链地址+红黑树。因为链地址法查找，
删除时间复杂度都为O(n)，红黑树为O(logn),更加高效。

&emsp;&emsp;jdk1.8中，链表的长度大于等于8，如果table.size<64，就会转为红黑树，否则只会扩容。在<=6的
时候退化成链表，之所以是6而不是7，是为了7和8相差太小，删除添加结点的时候，频繁的在树化和链化之间切换

&emsp;&emsp;可能有人会跟我一样产生这样的疑问，为什么不直接用红黑树，而是先用链表再考虑使用红黑树？

&emsp;&emsp;这是因为红黑树的插入删除，需要旋转，这需要时间。数据量少的情况下，反而链表效率更高，至于为什么偏偏是8，
可能是一个经过试验的经验值。